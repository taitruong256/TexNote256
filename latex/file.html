<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Truong Duc Tai (taitruong256)" />
  <title>CodeMMLU Challenge Technical Report</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">CodeMMLU Challenge Technical Report</h1>
<p class="author">Truong Duc Tai (taitruong256)</p>
</header>
<h1 id="introduction">Introduction</h1>
<p>Evaluating programming comprehension in large language models (LLMs)
remains a significant challenge. While many LLMs demonstrate strong code
generation capabilities, their ability to analyze, reason, and
understand software engineering principles is less explored. The
CodeMMLU Challenge provides a structured benchmark to assess programming
knowledge through multiple-choice questions covering topics such as code
analysis, debugging, and system design. Unlike traditional benchmarks
that focus on syntax and code completion, this evaluation aims to
measure deeper reasoning and problem-solving skills in software
development.</p>
<p>Several existing models have been optimized for technical reasoning.
GPT-4o builds upon OpenAI’s advancements in general AI reasoning, but
algorithmic problem-solving remains an open question. DeepSeek-V3 is
specifically trained on programming-related datasets, making it a strong
candidate for software engineering tasks. Google’s Gemini 2.0 Flash is
optimized for efficiency in code-related queries, while Llama 3 70B
Instruct offers transparency as an open-weight model.</p>
<p>Due to constraints in time and computational resources, training
models from scratch or fine-tuning them was not feasible for this study.
Instead, I evaluated existing pre-trained models using API-based
inference to ensure a practical and reproducible benchmarking process.
This approach allows for direct comparison of state-of-the-art models
under controlled conditions while maintaining consistency in input
format and evaluation methodology.</p>
<h1 id="methodology">Methodology</h1>
<figure id="fig:method">
<img src="method.png" style="width:80.0%" />
<figcaption>System architecture for model evaluation
workflow</figcaption>
</figure>
<p>The evaluation follows a structured four-stage process, as
illustrated in Figure <a href="#fig:method" data-reference-type="ref"
data-reference="fig:method">1</a>, consisting of prompt construction,
API-based inference, response validation, and result aggregation. This
pipeline ensures a standardized and robust assessment across multiple
models.</p>
<p>The process begins with prompt construction, where raw
multiple-choice questions from a CSV dataset are transformed into a
consistent format. Each prompt explicitly presents a question, four
answer choices, and a directive instructing models to return a
single-letter response (A, B, C, or D). This structured format
eliminates ambiguities and ensures that all models receive uniform
input.</p>
<p>During inference, API requests are executed in parallel for four
models: Deepseek-V3, Gemini 2.0 Flash, GPT-4o, and Llama 3 70B Instruct.
To enhance efficiency, questions are processed in batches of 5–10 per
request. The system incorporates robust error-handling mechanisms,
including automatic retries, to manage rate limits and network timeouts.
Each API call adheres to a predefined structure, including
authentication credentials and content headers, to maintain stable and
reliable communication with model endpoints.</p>
<p>Once responses are collected, they undergo systematic validation to
ensure compliance with the expected multiple-choice format. The system
extracts and records answers from JSON outputs, enforcing strict
adherence to the single-character response format. Any anomalies, such
as missing or improperly formatted responses, are logged for further
inspection. This step guarantees data integrity before performance
analysis.</p>
<p>In the final stage, a majority voting mechanism is applied to
reconcile differences in model predictions. When responses vary, the
system selects the most frequently chosen answer as the final decision.
If no clear majority exists, alternative resolution strategies, such as
confidence-weighted voting, may be employed to enhance stability. The
final results are compiled into a structured CSV output, containing
question identifiers and model-generated answers, with optional
confidence scores when available.</p>
<h1 id="execution-and-results">Execution and Results</h1>
<p>The results of the evaluation highlight significant differences in
model performance. Table <a href="#tab:results"
data-reference-type="ref" data-reference="tab:results">1</a> summarizes
the accuracy scores achieved by each model on the CodeMMLU
benchmark.</p>
<div id="tab:results">
<table>
<caption>Comparative model performance on CodeMMLU benchmark</caption>
<thead>
<tr>
<th style="text-align: left;"><strong>Model</strong></th>
<th style="text-align: center;"><strong>API Source</strong></th>
<th style="text-align: center;"><strong>Accuracy Score</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4o</td>
<td style="text-align: center;">OpenAI</td>
<td style="text-align: center;">0.70</td>
</tr>
<tr>
<td style="text-align: left;">DeepSeek-V3</td>
<td style="text-align: center;">OpenRouter</td>
<td style="text-align: center;">0.75</td>
</tr>
<tr>
<td style="text-align: left;">Gemini-2.0-Flash</td>
<td style="text-align: center;">Google AI</td>
<td style="text-align: center;">0.72</td>
</tr>
<tr>
<td style="text-align: left;">Llama 3 70B Instruct</td>
<td style="text-align: center;">Together AI</td>
<td style="text-align: center;">0.62</td>
</tr>
<tr>
<td style="text-align: left;">Voting Ensemble</td>
<td style="text-align: center;">Combination</td>
<td style="text-align: center;">0.71</td>
</tr>
</tbody>
</table>
</div>
<p>Among the models evaluated, DeepSeek-V3 achieved the highest accuracy
at 75%, indicating strong capabilities in programming-related reasoning
tasks. The variance in accuracy scores suggests differences in how each
model approaches software engineering problems. The ensemble method,
based on majority voting, achieved an accuracy of 71%, demonstrating its
effectiveness in leveraging multiple model outputs to improve
performance.</p>
<p>The evaluation pipeline processed all responses efficiently, with no
significant bottlenecks. The distribution of answer choices across
options (A, B, C, D) appeared balanced, indicating that models did not
exhibit biases toward specific choices.</p>
<p>Despite these results, there are notable limitations. The enforced
single-character response format does not capture the reasoning behind
model decisions, making it difficult to analyze why a particular answer
was chosen. Additionally, prompt length constraints may have limited the
amount of contextual information available for complex programming
questions.</p>
<h1 id="improvement-strategies">Improvement Strategies</h1>
<p>Several strategies can be employed to enhance model performance in
programming comprehension tasks.</p>
<h3 class="unnumbered" id="fine-tuning-on-programming-datasets">4.1
Fine-Tuning on Programming Datasets</h3>
<p>Training models on high-quality, domain-specific datasets can
significantly improve their reasoning capabilities. By incorporating
diverse programming problems, ranging from algorithmic challenges to
software engineering case studies, models can develop a deeper
understanding of computational logic and design principles.</p>
<h3 class="unnumbered" id="chain-of-thought-reasoning">4.2
Chain-of-Thought Reasoning</h3>
<p>Instead of directly predicting an answer, models can be guided to
generate intermediate reasoning steps before selecting a final response.
This approach encourages structured thinking and has been shown to
improve accuracy in tasks requiring logical inference.</p>
<h3 class="unnumbered" id="zero-shot-and-few-shot-learning">4.3
Zero-Shot and Few-Shot Learning</h3>
<p>Providing contextual examples of similar problems before querying the
model can enhance its generalization capabilities. Few-shot learning
methods, where a small number of example solutions are included in the
prompt, allow models to adjust their reasoning approach based on prior
demonstrations.</p>
<h3 class="unnumbered" id="optimized-ensemble-strategies">4.4 Optimized
Ensemble Strategies</h3>
<p>Beyond simple majority voting, more sophisticated ensemble methods
can be employed. Confidence-weighted voting assigns different levels of
trust to models based on their past performance in specific question
categories. This technique optimally integrates multiple model outputs
to increase reliability.</p>
<h1 id="conclusion">Conclusion</h1>
<p>This evaluation identified DeepSeek-V3 as the strongest model for
programming-related questions, achieving 75% accuracy under controlled
conditions. The analysis highlights key differences in technical
reasoning capabilities across models while identifying areas for
improvement. Future work can focus on fine-tuning models on specialized
datasets, applying chain-of-thought prompting, and optimizing ensemble
strategies to enhance performance in programming question-answering
tasks.</p>
</body>
</html>
